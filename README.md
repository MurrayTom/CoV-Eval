# CoV-Eval (A comprehensive multi-task evaluation benchmark for LLM code security)
This is the official implementation of "**Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective**", accepted to the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025) Dataset & Benchmark Track

## üåê Overview üåê
<div align="center">

<img src="assets/intro1.png" alt="Real-World Vulnerable Code Examples Generated by GPT-4o" style="width:70%;"/>

</div>

As Copilot-style assistants become widely adopted in software development, the security of LLM-generated code is becoming increasingly critical. A study found that 35.8% of code generated by GitHub Copilot contains common security vulnerabilities. Similar issues have been observed in other LLM-based code generation tools.

<div align="center">

<img src="assets/intro2.png" alt="Real-World Demand: Multi-Functional Code Copilots" style="width:90%;"/>

</div>

To evaluate the security of LLM-generated code, several representative benchmarks have been proposed, but they focus solely on code completion or generation, neglecting other essential tasks in real-world development. In actual software development, an ideal code copilot should automatically complete a series of tasks: Code Generation and Completion, Vulnerability Detection and Classification, Vulnerability Repair.

<div align="center">

<img src="assets/CoV-Eval.png" alt="Evaluation and Judgement Pipeline of CoV-Eval" style="width:90%;"/>

</div>

This study proposes CoV-Eval, a multi-task benchmark for code security evaluation in large language models (LLMs). CoV-Eval covers diverse task types and a broad range of code security vulnerabilities. We evaluate various general LLMs and code LLMs from a security perspective, focusing on vulnerabilities in generated code as well as their capabilities in vulnerability detection, classification and repair.

## üìö Resources
- **[Paper](https://aclanthology.org/2025.acl-long.849/):** Details the evaluation benchmark design and key experimental results.

## Contents
- [Install](#install)
- [Usage](#Usage)

## Install
You need to install some packages. We suggest that the python version >= 3.9
```shell
pip install -r requirement
```

## Usage
Next, we provide some reference scripts to guide how to evaluate the code security of LLMs based on CoV-Eval.

### Code Completion
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type code_completion \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```
```
CUDA_VISIBLE_DEVICES=1 python judge.py \
    --task_type code_completion \
    --model_name qwen2.5-7b-instruct \
    --eval_dir ./results/vulnerability_repair \
    --output_dir ./results \
    --judge_model_name gpt-4o \
    --judge_model_path gpt-4o \
    --api_key xxx \
    --api_base xxx
```

### Vulnerability Repair
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type vulnerability_repair \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```
```
CUDA_VISIBLE_DEVICES=1 python judge.py \
    --task_type vulnerability_repair \
    --model_name qwen2.5-7b-instruct \
    --eval_dir ./results/vulnerability_repair \
    --output_dir ./results \
    --judge_model_name gpt-4o \
    --judge_model_path gpt-4o \
    --api_key xxx \
    --api_base xxx
```

### Vulnerability Detection and Classification
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type vulnerability_detection_and_classification \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```


## üñäÔ∏è Citing Info

```bibtex
@article{mou2024cov,
  title={Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective},
  author={Mou, Yutao and Zhang, Shikun and Ye, Wei},
  journal={arXiv preprint arXiv:2505.10494},
  year={2025}
}
```
