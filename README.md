# CoV-Eval (A comprehensive multi-task evaluation benchmark for LLM code security)
This is the official implementation of "**Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective**", accepted to the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025) Dataset & Benchmark Track

## üìö Resources
- **[Paper](https://aclanthology.org/2025.acl-long.849/):** Details the evaluation benchmark design and key experimental results.
- **[Code](https://github.com/MurrayTom/CoV-Eval):** Details the source code and datasets.
- **[VC-Judge](https://github.com/MurrayTom/CoV-Eval):** Judge model develop by us for automatic code auditing in CoV-Eval

## üåê Overview üåê
<div align="center">

<img src="assets/intro1.png" alt="Real-World Vulnerable Code Examples Generated by GPT-4o" style="width:70%;"/>

</div>

As Copilot-style assistants become widely adopted in software development, the security of LLM-generated code is becoming increasingly critical. A study found that 35.8% of code generated by GitHub Copilot contains common security vulnerabilities. Similar issues have been observed in other LLM-based code generation tools.

<div align="center">

<img src="assets/intro2.png" alt="Real-World Demand: Multi-Functional Code Copilots"/>

</div>

To evaluate the security of LLM-generated code, several representative benchmarks have been proposed, but they focus solely on code completion or generation, neglecting other essential tasks in real-world development. In actual software development, an ideal code copilot should automatically complete a series of tasks: **Code Generation and Completion**, **Vulnerability Detection** and **Classification**, **Vulnerability Repair**.

<div align="center">

<img src="assets/CoV-Eval.png" alt="Evaluation and Judgement Pipeline of CoV-Eval" style="width:90%;"/>

</div>

This study proposes **CoV-Eval**, a multi-task benchmark for **code security evaluation of large language models (LLMs)**. CoV-Eval covers diverse task types and a broad range of code security vulnerabilities. We evaluate various general LLMs and code LLMs from a security perspective, focusing on vulnerabilities in generated code as well as their capabilities in vulnerability detection, classification and repair.


## üó°Ô∏è Code Security Vulnerability Covered in CoV-Eval üõ°Ô∏è

We provide brief descriptions of 18 common software security vulnerability types, based on the [CWE Top 25 (2023)](https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html).

| CWE ID | Vulnerability Name | Description |
|--------|--------------------|-------------|
| **CWE-22** | Improper Limitation of a Pathname to a Restricted Directory (*Path Traversal*) | Occurs when input allows access to files outside of an intended directory via relative paths (e.g., `../`). |
| **CWE-798** | Use of Hard-coded Credentials | Involves embedding credentials directly in source code, making them easily discoverable if the code is compromised. |
| **CWE-787** | Out-of-Bounds Write | Happens when a program writes data outside the boundaries of allocated memory, possibly overwriting critical data. |
| **CWE-732** | Incorrect Permission Assignment for Critical Resource | Occurs when improper permissions are granted to critical system resources, allowing unauthorized access. |
| **CWE-476** | NULL Pointer Dereference | A crash or undefined behavior that results when a program dereferences a `null` pointer. |
| **CWE-434** | Unrestricted Upload of File with Dangerous Type | Refers to insufficient validation during file uploads, allowing executable or malicious files to be uploaded. |
| **CWE-190** | Integer Overflow or Wraparound | Happens when arithmetic operations exceed the range of the integer type, leading to incorrect or dangerous results. |
| **CWE-125** | Out-of-Bounds Read | A vulnerability in which data is read past the end or before the beginning of a buffer, possibly revealing sensitive data. |
| **CWE-78** | Improper Neutralization of Special Elements used in an OS Command (*OS Command Injection*) | Malicious input can alter the intended execution of system-level commands. |
| **CWE-502** | Deserialization of Untrusted Data | Occurs when untrusted or malicious data is deserialized without validation, potentially leading to remote code execution. |
| **CWE-79** | Improper Neutralization of Input During Web Page Generation (*Cross-site Scripting*) | Involves failure to sanitize user inputs, allowing attackers to inject scripts into web pages. |
| **CWE-522** | Insufficiently Protected Credentials | Occurs when credentials are stored or transmitted insecurely, exposing them to unauthorized actors. |
| **CWE-20** | Improper Input Validation | A general vulnerability involving insufficient validation of user-supplied input, potentially leading to code injection, buffer overflows, etc. |
| **CWE-89** | SQL Injection: Improper Neutralization of Special Elements | Occurs when attackers can inject malicious SQL commands into queries due to insufficient input sanitization. |
| **CWE-200** | Exposure of Sensitive Information to an Unauthorized Actor | Involves unintended disclosure of sensitive information to entities not authorized to receive it. |
| **CWE-119** | Improper Restriction of Operations within the Bounds of a Memory Buffer | A broad category covering buffer overflows and underflows due to mismanagement of memory access. |
| **CWE-416** | Use After Free | Happens when memory that has already been freed is accessed, potentially leading to crashes or arbitrary code execution. |
| **CWE-306** | Insufficient Processing of Invalid or Unintended Input | Refers to the failure to handle invalid or unexpected input correctly, often leading to broader vulnerabilities. |


## Contents
- [Install](#install)
- [Usage](#Usage)

## Install
You need to install some packages. We suggest that the python version >= 3.9
```shell
pip install -r requirement
```

## Usage
Next, we provide some reference scripts to guide how to evaluate the code security of LLMs based on CoV-Eval.

### Code Completion
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type code_completion \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```
```
CUDA_VISIBLE_DEVICES=1 python judge.py \
    --task_type code_completion \
    --model_name qwen2.5-7b-instruct \
    --eval_dir ./results/vulnerability_repair \
    --output_dir ./results \
    --judge_model_name gpt-4o \
    --judge_model_path gpt-4o \
    --api_key xxx \
    --api_base xxx
```

### Vulnerability Repair
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type vulnerability_repair \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```
```
CUDA_VISIBLE_DEVICES=1 python judge.py \
    --task_type vulnerability_repair \
    --model_name qwen2.5-7b-instruct \
    --eval_dir ./results/vulnerability_repair \
    --output_dir ./results \
    --judge_model_name gpt-4o \
    --judge_model_path gpt-4o \
    --api_key xxx \
    --api_base xxx
```

### Vulnerability Detection and Classification
```
CUDA_VISIBLE_DEVICES=1 python evaluation.py \
    --task_type vulnerability_detection_and_classification \
    --model_name qwen2.5-7b-instruct \
    --model_path /home/xxx/Models/Qwen2.5-7B-Instruct \
    --save_dir ./results
```


## üñäÔ∏è Citing Info

```bibtex
@article{mou2024cov,
  title={Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective},
  author={Mou, Yutao and Zhang, Shikun and Ye, Wei},
  journal={arXiv preprint arXiv:2505.10494},
  year={2025}
}
```
